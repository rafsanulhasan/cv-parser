# .cursorrules - CV Parser

> **ðŸ“– Master Reference:** See [ARCHITECTURE.md](./ARCHITECTURE.md) for complete implementation details.

## Quick Reference

**Tech Stack:** Angular 17 | Node.js/Express | Multi-Provider AI (Browser/Ollama/OpenAI) | IndexedDB  
**File Formats:** PDF, DOCX, TXT, MD, JSON  
**Key Services:** ModelRegistry, Ollama, OpenAI, Embedding, LocalExtraction, Storage, FileParsing

## Core Rules

## Core Rules

### 1. Multi-Provider Architecture
```typescript
// ALWAYS check provider before delegating
const provider = this.modelRegistry['selectedProviderSubject'].value;

if (provider === 'ollama') {
  return this.ollamaService.method();
} else if (provider === 'browser') {
  return this.browserMethod();
} else if (provider === 'openai') {
  return this.openAIService.method();
}
```

### 2. Ollama Multi-Layer Downloads
```typescript
// REQUIRED: Track progress per digest
const digestProgress = new Map<string, number>();

for await (const _ of ollamaService.pullModel(modelId, 
  (completed, total, digest, status) => {
    if (digest) digestProgress.set(digest, completed);
    const totalDownloaded = Array.from(digestProgress.values()).reduce((a,b) => a+b, 0);
    const percent = Math.min(100, Math.round((totalDownloaded / (total * digestProgress.size)) * 100));
  }
)) {}
```

### 3. WebLLM Engine Lifecycle
```typescript
// ALWAYS try reload before recreate
if (this.engine) {
  try {
    await this.engine.reload(modelId);
    return;
  } catch {
    await this.engine.unload();
    this.engine = null;
    await new Promise(r => setTimeout(r, 500)); // GPU reset
  }
}
this.engine = await CreateMLCEngine(modelId, { initProgressCallback });
```

### 4. IndexedDB
- **Database:** `ai-vector-db` (version 4)
- **Store:** `documents` (keyPath: `requestId`)
- **Upgrade:** Drop and recreate (data loss acceptable for dev)

### 5. Angular
- **Modular Components**: Split large UIs into focused sub-components (`components/ui` vs `components/feature`)
- Standalone Components with explicit imports
- RxJS BehaviorSubjects for state
- Inline templates and styles
- Step-level error tracking

### 6. UI/UX
- Max-width 800px, centered
- File colors: PDF(red), DOCX(blue), TXT(gray), MD(purple), JSON(green)
- Provider tabs: Browser | Ollama | OpenAI
- Model grouping: Installed vs Available
- **Confirmation**: NO `window.confirm()`. Use 2-step UI buttons.

### 7. State Persistence
localStorage: `selectedProvider`, `selectedEmbeddingModel`, `selectedChatModel`, `openaiApiKey`, `ollamaApiUrl`, `ollamaApiKey`

### 8. Backend
- `/models/*` - HF proxy with auth
- `/upload` - Multer to `uploads/`
- `/extract` - MCP server (OpenAI gpt-4o-mini)
- Transformers.js: `env.remoteHost = 'http://localhost:3000/models/'`

### 10. Build Configuration (Critical)
- **Node Polyfills**: Angular build fails on `sharp`/`onnxruntime-node`.
- **Fix**: Exclude in `package.json` (`browser: false`) AND map to mock in `tsconfig.json`.
- **Budget**: Increase `maximumError` to 10mb for ML libs.

### 9. Error Handling
- Graceful degradation per provider
- Step-level error icons (âœ—)
- Console logging for debugging
- User-friendly messages

### 10. Code Style
- TypeScript strict typing
- 2 spaces, semicolons
- Descriptive names
- Comment complex logic

## Common Tasks

**Add Provider:**
1. Create service with `generate()` and `getEmbeddings()`
2. Add to `ModelProvider` type
3. Update Embedding/Extraction services
4. Add UI tab + config inputs

**Add File Format:**
1. Update `FileParsingService.parseFile()` switch
2. Implement parser method
3. Update `getFileColor()`
4. Add to file input `accept`

**Modify Schema:**
1. Update prompt in `LocalExtractionService`
2. Update UI template
3. Consider IndexedDB version bump

## Development
```bash
cd frontend/web && npm start          # Frontend :4200
cd backend && npm start  # Backend :3000
ollama serve                        # Ollama mode
```

**Reference:** See [ARCHITECTURE.md](./ARCHITECTURE.md) for complete documentation.
